<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8" />
	<meta content="width=device-width,initial-scale=1.0,maximum-scale=1.0" name="viewport">
	<meta name="twitter:widgets:theme" content="light">
	<meta property="og:title" content="Your-Title-Here" />
	<meta property="og:type" content="website" />
	<meta property="og:image" content="Your-Image-Url" />
	<meta property="og:description" content="Your-Page-Description" />
	<title>Child-View</title>
	<link rel="shortcut icon" type="image/x-icon" href="images/theme-mountain-favicon.ico">

	<!-- Font -->
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:300,400,700%7CMontserrat:400,700%7CPlayfair+Display:400,400italic' rel='stylesheet' type='text/css'>
	<link href="https://fonts.googleapis.com/icon?family=Material+Icons"  rel="stylesheet">
	
	<!-- Css -->
	<link rel="stylesheet" href="css/core.min.css" />
	<link rel="stylesheet" href="css/skin.css" />

	<!--[if lt IE 9]>
    	<script type="text/javascript" src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

</head>
<body class="shop home-page">

	<!-- Side Navigation Menu -->
	<aside class="side-navigation-wrapper enter-right" data-no-scrollbar data-animation="push-in">
		<div class="side-navigation-scroll-pane">
			<div class="side-navigation-inner">
				<div class="side-navigation-header">
					<div class="navigation-hide side-nav-hide">
						<a href="#">
							<span class="icon-cancel medium"></span>
						</a>
					</div>
				</div>
				<nav class="side-navigation">
					<ul>
						<li>
							<a href="about.html" class="contains-sub-menu">About Child-View</a>
							<ul class="sub-menu">
								
								<li>
									<a href="about.html#Publications">Publications</a>
								</li>
								<li>
									<a href="about.html#Participants">Who We Are</a>
								</li>
							</ul>
						</li>
						<li>
							<a href="document.html" class="contains-sub-menu">Build Your Own Dataset</a>
							<ul class="sub-menu">
								<li>
									<a href="index.html">0. Intro</a>
								</li>
								<li>
									<a href="index.html">1. Setup TDW</a>
								</li>
								<li>
									<a href="index.html">2. Core Concepts</a>
								</li>
								<li>
									<a href="index.html">3. Scene Setup</a>
								</li>
								<li>
									<a href="index.html">4. Avatar Setup</a>
								</li>
								<li>
									<a href="index.html">5. Generating Dataset</a>
								</li>
								<li>
							</ul>
						</li>
						<li>
							<a href="datasets.html" class="contains-sub-menu">Datasets</a>
							<ul class="sub-menu">
								<li>
									<a href="index-house.html">Archviz House</a>
								</li>
								<li>
									<a href="index-house-light.html">Archviz House with Lighting Conditions</a>
								</li>
								<li>
									<li>
									<a href="index-apartment.html">Apartment</a>
								</li>
							</ul>
						</li>
						<li>
							<a href="https://pennstate.qualtrics.com/jfe/form/SV_4VqSFRybglbV59Y">Feedback Form</a>
						</li>
					</ul>
				</nav>
				<div class="side-navigation-footer">
					<p class="copyright no-margin-bottom">&copy; The Pennsylvania State University. All Rights Reserved.</p>
				</div>
			</div>
		</div>
	</aside>
	<!-- Side Navigation Menu End -->

	<div class="wrapper">
		<div class="wrapper-inner">

			<!-- Header -->
			<header class="header header-fixed header-fixed-on-mobile header-transparent" data-bkg-threshold="100" data-compact-threshold="100">
				<div class="header-inner">
					<div class="row nav-bar">
						<div class="column width-12 nav-bar-inner">
							<div class="logo">
								<div class="logo-inner">
									<a href="index.html"><img src="images/logo-dark.png" alt="Child-View Logo" /></a>
									<a href="index.html"><img src="images/logo.png" alt="Child-View Logo" /></a>
								</div>
							</div>
							<nav class="navigation nav-block secondary-navigation nav-right">
								<ul>
									<li class="aux-navigation hide">
										<!-- Aux Navigation -->
										<a href="#" class="navigation-show side-nav-show nav-icon">
											<span class="icon-menu"></span>
										</a>
									</li>
								</ul>
							</nav>
							<nav class="navigation nav-block primary-navigation nav-right">
								<ul>
									<li>
										<a href="about.html" style="font-size: 10pt;">About Child-View</a>
										<ul class="sub-menu">
											
											<li>
												<a href="about.html#Publications">Publications</a>
											</li>
											<li>
												<a href="about.html#Participants">Who We Are</a>
											</li>
										</ul>
									</li>
									<li>
										<a href="document.html" style="font-size: 10pt;">Build Your Own Dataset</a>
										<ul class="sub-menu">
											<li>
												<a href="index.html">0. Intro</a>
											</li>
											<li>
												<a href="index.html">1. Setup TDW</a>
											</li>
											<li>
												<a href="index.html">2. Core Concepts</a>
											</li>
											<li>
												<a href="index.html">3. Scene Setup</a>
											</li>
											<li>
												<a href="index.html">4. Avatar Setup</a>
											</li>
											<li>
												<a href="index.html">5. Generating Dataset</a>
											</li>
											<li>
										</ul>
									</li>
									<li class="contains-mega-sub-menu">
										<a href="datasets.html" style="font-size: 10pt;">Datasets</a>
										<ul class="mega-sub-menu row">
											<li class="column width-4">
												<a href="index-house.html">Archviz House</a>
												<ul>
													<li>
														<a href="hero-house14k.html">14k dataset</a>
													</li>
													<li>
														<a href="hero-house100k.html">100k dataset</a>
													</li>
												</ul>
											</li>
											<li class="column width-4">
												<a href="index-house-light.html">Archviz House with Lighting Conditions</a>
												<ul>
													<li>
														<a href="hero-house-Blue.html">Blue_grotto_8k</a>
													</li>
													<li>
														<a href="hero-house-Indoor.html">Indoor_pool_4k</a>
													</li>
													<li>
														<a href="hero-house-Kiara.html">Kiara_1_dawn_4k</a>
													</li>
													<li>
														<a href="hero-house-Mosaic.html">Mosaic_tunnel_4k</a>
													</li>
													<li>
														<a href="hero-house-Ninomaru.html">Ninomaru_teien_4k</a>
													</li>
													<li>
														<a href="hero-house-Royal.html">Royal_esplanade_4k</a>
													</li>
													<li>
														<a href="hero-house-Small.html">Small_hangar_01_4k</a>
													</li>
													<li>
														<a href="hero-house-Venice.html">Venice_sunrise_4k</a>
													</li>
													<li>
														<a href="hero-house-Whipple.html">Whipple_creek_gazebo_4k</a>
													</li>
												</ul>
											</li>
											<li class="column width-4">
												<a href="index-apartment.html">Apartment</a>
												<ul>
													<li>
														<a href="hero-apartment14k.html">14k dataset</a>
													</li>
													<li>
														<a href="hero-apartment100k.html">100k dataset</a>
													</li>
												</ul>
											</li>
										</ul>	
									</li>
									<li>
										<a href="https://pennstate.qualtrics.com/jfe/form/SV_4VqSFRybglbV59Y" style="font-size: 10pt;">Feedback Form</a>
									</li>
								</ul>
							</nav>
						</div>
					</div>
				</div>
			</header>
			<!-- Header End -->

			<!-- Content -->
			<div class="content clearfix">

				<!-- Fullscreen Slider Section -->
				<section class="section-block featured-media tm-slider-parallax-container">
					<div class="tm-slider-container full-width-slider" data-featured-slider data-parallax data-scale-under="960">
						<ul class="tms-slides">
							<li class="tms-slide" data-image data-force-fit data-overlay-bkg-color="#000000" data-overlay-bkg-opacity="0.70">
								<div class="tms-content">
									<div class="tms-content-inner center v-align-middle">
										<div class="row">
											<div class="column width-12">
												<h1 class="title-xlarge font-alt-2 weight-light color-white mb-20">
													<span class="tms-caption" data-animate-in="preset:slideInDownShort;duration:800ms;delay:200;" data-no-scale>Child-View</span>
												</h1>
												<div class="clear"></div>
												<p class="tms-caption lead mb-30 color-grey-ultralight"
													data-animate-in="preset:slideInUpShort;duration:900ms;delay:200ms;"
													data-no-scale
													>A Visual Deep Learning Database Built from Navigating Virtual Environments </p>
												<div class="clear"></div>
											</div>
										</div>
									</div>
								</div>
								<img data-src="images/tdw/hero-screen.png" src="images/blank.png" />
							</li>
						</ul>
					</div>
				</section>
				<!-- Fullscreen Slider Section End -->

				<!-- Feature Section 2 -->
				<section id="block-design" class="section-block feature-2">
					<div class="column width-10 offset-1 center">
						<h2 class="mb-50">A visual deep-learning datasets inspired by cognitive development </h2>
					</div>
					<div class="row flex">
						<div class="column width-5 push-6">
							<div class="feature-image mb-mobile-50">
								<div class="feature-image-inner center-on-mobile horizon" data-animate-in="preset:slideInRightShort;duration:1000ms;" data-threshold="0.4">
									<img src="images/GraphicalAbstract.png" alt="Graphical abstract of Child-View Project" title="Graphical abstract of Child-View Project" />
								</div>
							</div>
						</div>
						<div class="column width-7 pull-6">
							<div class="feature-content">
								<div class="feature-content-inner horizon center-on-mobile" data-animate-in="preset:slideInUpShort;duration:900ms;delay:300ms;" data-threshold="0.4">
									<p class="MB-50">Many visual learning databases consist of millions if not billions of distinct images which are used to train algorithms and models. But these databases far exceed the standard visual exposure of a typical human toddler and yet the end result of such training even with our most advanced algorithms still falls short of human visual performance on simple classification tasks (REF Imagenet V2).  Making datasets even larger may improve performance but there are diminishing returns with increasing training size.  What is more, training on billions of images is outside of the reach of small and mid-tier research programs, which limits the ability of both industry and the scholarly community to develop better algorithms.<br/><br/>
										In Child-view, we aim to shift the focus of deep learning toward optimizing what learning can be accomplished with small data sets that are closer in scope and size to the visual experience of a human child’s first years of life. There is much to be learned about visual learning from children, who tap into a rich source of environmental information about how their body deliberately samples information through controlled orientation of its body and senses and interactions with the world (Ballard et al 1997; Campos et al. 2000; Smith 2005).<br/><br/>
										This method of exploration enables them to develop a robust mode of the visual world despite inhabiting an environment with very limited distributions. Many children in modern households spend nearly the first two years of their life inside one or two buildings, viewing a limited set of spaces, surfaces, faces, and objects from many directions and under a wide variety of lighting conditions. In fact, headcam data reveals that only three specific faces make up nearly all of the exposure to faces for the average child from western households in the first year of their life (Jayaraman, et al. 2015). <br/><br/> 
									</p>
								</div>
							</div>
						</div>
					</div>
					<div class="row flex" style="margin-top: 100px;">
						<div class="column width-7 push-6">
							<div class="feature-content">
								<div class="feature-content-inner horizon center-on-mobile" data-animate-in="preset:slideInUpShort;duration:900ms;delay:300ms;" data-threshold="0.4">
									<p class="MB-50">
										How do people learn with such input?  It could be that perceiving within a consistent environment, in which objects persist over time and through a variety of changes of perspective, lighting condition, etc, form richer learning environments than large image sets. The ways that light is reflected, absorbed, refracted, or scattered in the physical world provides a richer set of augmentations that are based on the laws of physics. Augmentation of 2-D images cannot capture these changes because they lack the physical properties of interacting with light rays in a three-dimensional space (real or virtual).  <br/><br/>
										It has been demonstrated that the ability to learn spatially invariant visual representations of objects is dependent on the temporal adjacency of views during an eye movement (Li & Dicarlo 2008) or exposure to temporally smooth objects (Wood & Wood 2016; Wood & Wood 2018) which would allow the visual stream to learn that different views correspond to the same object. Child-view extends this idea to movement through an environment, which extends the transformations to include changes in lighting across time and changes in perspective within a scene that can teach the learner about shadows and occlusion. The viewer uses their own perception of self-motion and time as a means to sort sensory information according to where and when it was sampled. This environmental learning process turns the statistical consistency of the world, which would be a disadvantage from a standard deep learning approach, into a crucial signal for learning about the physical properties of how light and materials interact in a fully populated visual environment.<br/><br/>  
									</p>
								</div>
							</div>
						</div>
						<div class="column width-4 pull-8">
							<div class="feature-image mb-mobile-50">
								<div class="feature-image-inner center-on-mobile horizon" data-animate-in="preset:slideInRightShort;duration:1000ms;" data-threshold="0.4">
									<img src="images/imgdif.png" alt="Graphical abstract of Child-View Project" title="Graphical abstract of Child-View Project" />
								</div>
							</div>
						</div>
					</div>
					<div class="row" style="margin-top: 100px;">
						<div class="column center">
							<p class="mb-50">
								Given a large set of images with spatial and temporal contiguity information, a self-supervised contrastive learning approach could use an objective function that relies on the spatiotemporal proximity of two images as a graded similarity signal of any two views. Our self-supervised algorithm assumes that training uses temporal and spatial contiguity as a proxy for similarity, rather than assessing similarity of the pixelwise content.  <br/><br/>
								Using space and time allows us to tag images as similar regardless of their apparent content.  Instead of attempting to discern whether two images are augmented versions of the same original image, the objective function maximizes a new contrastive function according to the relative temporal and spatial separation between a key image and other images sampled from the environment. Both space and time provide partially independent sources of similarity that augment the learning process. The image sets we provide here will facilitate the development of new approaches to computer vision that allow us to learn more effectively with smaller datasets rather than building exhaustively large datasets.   <br/><br/>
								The images provided here are all generated from the ThreeDWorlds engine.  You can also generate your own images by following a step-by-step tutorial provided in this website. <br/><br/>
							</p>
							<a href="datasets.html" class="button medium pill border-grey-light bkg-hover-theme color-grey color-hover-white"data-animate-in="preset:scaleOut;duration:900ms;delay:400ms;">View Datasets</a>
						</div>
					</div>
				</section>
				<!-- Feature Section 2 End -->

				<!-- Feature Column Section -->
				<div id="discover" class="section-block replicable-content bkg-grey-ultralight">
					<div class="row">
						<div class="column width-8 offset-2 center">
							<h2 class="mb-50">The Benefits of Child-View</h2>
						</div>
					</div>
					<div class="row flex">
						<div class="column width-4">
							<div class="box small bkg-grey-ultralight">
								<div class="feature-column small mb-50 left">
									<span class="feature-icon color-theme material-icons">psychology</span>
									<div class="feature-text">
										<h4>Psychological Principles</h4>
										<p>Child-View Project is inspired by principles of Cognitive Development, so the researcher can have better comparison to infant visual learning </p>
									</div>
								</div>
							</div>
						</div>
						<div class="column width-4">
							<div class="box small bkg-grey-ultralight">
								<div class="feature-column small left">
									<span class="feature-icon color-theme material-icons">post_add</span>
									<div class="feature-text">
										<h4>Environments</h4>
										<p>Child-View Project can Improved self-supervised visual learning even with smaller image databases since environments of Child-View Project datasets are treated as additional datasources. </p>
									</div>
								</div>
							</div>
						</div>
						<div class="column width-4">
							<div class="box small bkg-grey-ultralight">
								<div class="feature-column small mb-50 left">
									<span class="feature-icon color-theme material-icons">announcement</span>
									<div class="feature-text">
										<h4>Accessibility</h4>
										<p>Child-View Project has increased accessibility for visual deep learning for early career researchers.</p>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
				<!-- Feature Column Section End -->

				<!-- Service Section -->
				<section class="section-block replicable-content">
					<div class="row">
						<div class="column width-8 offset-2 center">
							<h2 class="mb-30">Why is Developmental Psychology important for computer vision?</h2>
						</div>
						<div class="column width-10 offset-1 left">
							<p class="mb-50">
								The historically popular approaches to deep learning (DL) of visual categories are dissimilar to how humans learn.<br/><br/>
								First, algorithms are exposed to thousands of examples of a given object type in randomly shuffled order, and these images are devoid of context beyond the background. This training experience is essentially opposite to the visual experience of a human child, which has extensive exposure to a narrow range of specific visual objects within a highly familiar and constrained context. Many children in modern households spend nearly the first two years of their life inside one or two buildings, viewing a limited set of spaces, surfaces, faces, and objects from many directions and under a wide variety of lighting conditions. In fact, headcam data reveals that only three specific faces make up nearly all of the exposure to faces for the average child from western households in the first year of their life (Jayaraman, et al. 2015).<br/><br/>
								Moreover, infants see a comparatively small number of objects, and many of these are viewed exclusively within a given context, such as a toaster being only on a specific kitchen counter with the same wall texture. This extensive exposure to a narrow range of objects with highly specific contexts would produce a poor performance for DL as conventional machine learning approaches typically require balanced exposure to a wide range of objects in different backgrounds to avoid learning inappropriate statistical relationships. For example, if a toaster sits in front of a tile background on the kitchen counter, a typical DL algorithm would learn that the tile is typically indicative of a toaster and be unable to recognize a toaster in a different setting because they have no representation of surfaces or the notion that objects exist.  Large datasets attempt to mitigate this problem by presenting many exemplars of objects on a wide range of backgrounds and with the use of data augmentation techniques which artificially alter the background in the training set. But human children learn to parse the visual environment very effectively without such diverse visual experiences. It should be possible to build systems that can learn from data sets that are smaller and lack a wide variety of object/background pairings.   
							</p>
						</div>
					</div>
				</section>
				<!-- Service Section End -->

				<!-- Hero 5 Section -->
				<div class="section-block replicable-content bkg-grey-ultralight">
					<div class="row">
						<div class="column width-8 offset-2 center">
							<h2 class="mb-50">What is self-supervised learning?</h2>
						</div>
						<div class="column width-10 offset-1 left">
							<p class="mb-50">Recent innovations in computer vision have focused on self-supervised learning (i.e., sets of images without human-generated labels).  A common self-supervised approach is contrastive learning in which a network is trained to tell whether two given images are altered versions of the same image or different images. By forcing a network to learn that two alterations of the same image are from the same input, the network learns to build representations that correspond to the meaningful content of the image, rather than the surface properties of the pixel patterns. Self-supervised approaches use massive data sets pulled from the internet with no labels and used for training, such as MoCo (He et al., 2020), SimCLR (Chen et al., 2020), and BYOL (Grill et al., 2020). <br/><br/>
								Self-supervised learning approaches build general-purpose vision representations that are useful for other downstream tasks, such as building an image classifier.  To do this, one first trains the deep part of the network on a large set (typically hundreds of millions to billions) of unlabeled images to build a basic set of representations using an unsupervised learning function.  The goal of such learning is to build a set of representations that map high-dimensional patterns onto lower-dimensional representations that can be used for other tasks.  In a sense, this hybrid approach of self-supervised followed by supervised learning is similar to how children learn about the visual world. Their first period of life is spent exploring without labels, and their language faculties then come online and allow them to apply labels.  
							</p>
						</div>
					</div>
				</div>
				<!-- Hero 5 Section End -->

				<!-- Feature Column Section -->
				<div id="discover" class="section-block replicable-content">
					<div class="row">
						<div class="column width-8 offset-2 center">
							<h2 class="mb-30">What is the value of using small datasets?</h2>
						</div>
						<div class="column width-10 offset-1 left">
							<p class="mb-50">Modern visual learning usually relies on massive databases of different objects. But these databases are often unorganized in terms of environmental information, far exceed the level of visual information a human toddler experiences, and lead to minimal improvements in classification as databases grow exponentially larger. Child-view circumvents these limitations by relying on shared environmental context within its database.  [keep writing] </p>
						</div>
					</div>
					<div class="row flex">
						<div class="column width-4">
							<div class="box small bkg-white">
								<div class="feature-column small mb-50 left">
									<span class="feature-icon icon-map color-theme"></span>
									<div class="feature-text">
										<h4>8 Homepages</h4>
										<p>Warhol comes with 8 different homepages and 8 unique hero sections that can easily be dropped into any section of any page.</p>
									</div>
								</div>
							</div>
						</div>
						<div class="column width-4">
							<div class="box small bkg-white">
								<div class="feature-column small left">
									<span class="feature-icon icon-add-to-list color-theme"></span>
									<div class="feature-text">
										<h4>20+ Components</h4>
										<p>Make any design stand out by using a combination of the 20+ components that come with Warhol. Accordions, tabs, buttons and much more.</p>
									</div>
								</div>
							</div>
						</div>
						<div class="column width-4">
							<div class="box small bkg-white">
								<div class="feature-column small mb-50 left">
									<span class="feature-icon icon-power-plug color-theme"></span>
									<div class="feature-text">
										<h4>8+ Plugins</h4>
										<p>As with all our templates, Warhol comes with 8 premium in-house developed plugins, which means that when there's an update, you'll get it.</p>
									</div>
								</div>
							</div>
						</div>
						<div class="column width-4">
							<div class="box small bkg-white">
								<div class="feature-column small left">
									<span class="feature-icon icon-grid color-theme"></span>
									<div class="feature-text">
										<h4>Block Based</h4>
										<p>Warhol, like all our templates, is built using a block based, responsive framework that makes designing easier. Move blocks around and get a new design.</p>
									</div>
								</div>
							</div>
						</div>
						<div class="column width-4">
							<div class="box small bkg-white">
								<div class="feature-column small mb-50 left">
									<span class="feature-icon icon-credit color-theme"></span>
									<div class="feature-text">
										<h4>Ultimate Startup Pack</h4>
										<p>Warhol is a great starting point for startups that need a quick and simple solution when it comes to creating a site. Save both time &amp; money.</p>
									</div>
								</div>
							</div>
						</div>
						<div class="column width-4">
							<div class="box small bkg-white">
								<div class="feature-column small left">
									<span class="feature-icon icon-infinity color-theme"></span>
									<div class="feature-text">
										<h4>Infinite Possibilities</h4>
										<p>With a range of pre-design content blocks, components and plugins, Sarte provides you with an unlimited number of possibilities.</p>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
				<!-- Feature Column Section End -->

				<!-- Feature Section 2 -->
				<section class="section-block bkg-charcoal feature-2 right">
					<div class="row flex">
						<div class="column width-6">
							<div class="feature-image mb-mobile-50">
								<div class="feature-image-inner center-on-mobile horizon" data-animate-in="preset:slideInRightShort;duration:1000ms;" data-threshold="0.6">
									<img src="images/tdw/apartment.png" alt="Sample Image of Apartment environment" title="Sample Image of Apartment environment"/>
								</div>
							</div>
						</div>
						<div class="column width-5 push-1">
							<div class="feature-content">
								<div class="feature-content-inner left center-on-mobile horizon" data-animate-in="preset:slideInUpShort;duration:900ms;delay:300ms;" data-threshold="0.6">
									<h2 class="mb-30 color-white">Building your own child-view database is easy! </h2>
									<!-- <p class="lead font-alt-2 color-white opacity-06">Building your own child-view database is easy! </p> -->
									<p class="color-white opacity-06">Want to make your own egocentric database in a TDW virtual environment to further improve visual deep learning? <br/>
										Follow our step-by-step guide to build your own egocentric dataset in a TDW virtual environment so you can begin to investigate your own visual deep learning questions! </p>
									<a href="document.html" class="button medium pill border-grey-light bkg-hover-theme color-grey-light color-hover-white fade-location" data-animate-in="preset:scaleOut;duration:900ms;delay:400ms;"
									>View documentation</a>
								</div>
							</div>
						</div>
					</div>
				</section>
				<!-- Feature Section 2 End -->

				

			</div>
			<!-- Content End -->

			<!-- Footer -->
			<footer class="footer footer-fixed">
				<div class="footer-bottom">
					<div class="row">
						<div class="column width-12">
							<div class="footer-bottom-inner center">
								<div class="row">
									<p class="copyright pull-left clear-float-on-mobile">
										&copy; The Pennsylvania State University. All Rights Reserved.
									</p>
								</div>
							</div>
						</div>
					</div>
				</div>
			</footer>
			<!-- Footer End -->

		</div>
	</div>

	<!-- Js -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
	<script src="http://maps.googleapis.com/maps/api/js?v=3"></script>
	<script src="js/timber.master.min.js"></script>
</body>
</html>